# ü§ñ **Configuraci√≥n de Proveedores LLM**

Este proyecto ahora soporta **m√∫ltiples proveedores de LLM** con fallback autom√°tico:

## üéØ **Proveedores Configurados**

### **1. DeepSeek (Proveedor Principal)**
- **Prioridad**: ü•á Principal
- **Modelos**: `deepseek-chat`, `deepseek-coder`
- **Ventajas**: R√°pido, econ√≥mico, excelente para trading
- **API Key**: https://platform.deepseek.com/

### **2. OpenRouter (Proveedor Secundario)**
- **Prioridad**: ü•à Fallback
- **Modelos**: ‚úÖ **MODELOS GRATUITOS**
- **Ventajas**: Sin costo, m√∫ltiples modelos open-source
- **API Key**: https://openrouter.ai/keys

---

## üöÄ **Setup R√°pido**

### **1. Obtener API Keys**

#### **DeepSeek** (Recomendado)
```bash
1. Ir a: https://platform.deepseek.com/
2. Crear cuenta
3. Generar API Key
4. Copiar la key
```

#### **OpenRouter** (Gratis)
```bash
1. Ir a: https://openrouter.ai/keys
2. Crear cuenta con Google/GitHub
3. Generar API Key (gratis)
4. Copiar la key
```

### **2. Configurar Variables de Entorno**

#### **Windows (PowerShell)**
```powershell
# Crear archivo .env
Copy-Item .env.example .env

# Editar .env y agregar tus keys:
# DEEPSEEK_API_KEY=tu-key-aqui
# OPENROUTER_API_KEY=tu-key-aqui
```

#### **Linux/Mac**
```bash
# Copiar template
cp .env.example .env

# Editar .env
nano .env

# O usar export
export DEEPSEEK_API_KEY="tu-key-aqui"
export OPENROUTER_API_KEY="tu-key-aqui"
```

### **3. Verificar Configuraci√≥n**
```bash
python llm_config.py
```

Deber√≠as ver:
```
‚úÖ DeepSeek: Connected
  ‚Ä¢ deepseek-chat
  ‚Ä¢ deepseek-coder

‚úÖ OpenRouter: Connected
  ‚Ä¢ llama-3.1-8b (FREE)
  ‚Ä¢ mistral-7b (FREE)
  ‚Ä¢ gemma-2-9b (FREE)
```

---

## üé® **Modelos Disponibles**

### **DeepSeek Models**
| Modelo | Uso | Tokens | Costo |
|--------|-----|--------|-------|
| `deepseek-chat` | Trading decisions, an√°lisis general | Hasta 4K | ~$0.14/M tokens |
| `deepseek-coder` | Generaci√≥n de c√≥digo, scripts | Hasta 16K | ~$0.14/M tokens |

### **OpenRouter Free Models**
| Modelo | Proveedor | Versi√≥n | Tama√±o | Tools | Gratis |
|--------|-----------|---------|--------|-------|--------|
| `gemini-2.0-flash` | Google | 2.0 | - | ‚úÖ | ‚úÖ S√≠ |
| `gemini-2.0-flash-thinking` | Google | 2.0 | - | ‚úÖ | ‚úÖ S√≠ |
| `qwen-3-235b` | Alibaba | 3.0 | 235B params | ‚úÖ | ‚úÖ S√≠ |
| `phi-4` | Microsoft | 4.0 | 14B params | ‚úÖ | ‚úÖ S√≠ |
| `llama-3.3-70b` | Meta | 3.3 | 70B params | ‚úÖ | ‚úÖ S√≠ |
| `llama-3.1-405b` | Meta | 3.1 | 405B params | ‚úÖ | ‚úÖ S√≠ |
| `qwen-2.5-72b` | Alibaba | 2.5 | 72B params | ‚úÖ | ‚úÖ S√≠ |
| `phi-3.5-mini` | Microsoft | 3.5 | 3.8B params | ‚úÖ | ‚úÖ S√≠ |

**Notas:**
- ‚úÖ **Gemini 2.0**: √öltimo modelo de Google, excelente para multimodal + reasoning
- ‚úÖ **Qwen 3**: 235B par√°metros, uno de los modelos open-source m√°s grandes
- ‚úÖ **Phi-4**: Nuevo modelo peque√±o pero potente de Microsoft
- ‚úÖ **Llama 3.1 405B**: El modelo Llama m√°s grande disponible gratis

---

## üíª **Uso en C√≥digo**

### **B√°sico**
```python
from llm_config import get_llm_response

# Usar DeepSeek (principal)
response = get_llm_response(
    "Analiza este portfolio...",
    provider="deepseek",
    model="deepseek-chat"
)

# Usar OpenRouter (gratis)
response = get_llm_response(
    "Analiza este portfolio...",
    provider="openrouter",
    model="llama-3.1-8b"
)
```

### **Con Fallback Autom√°tico**
```python
from llm_config import llm_config

# Intenta DeepSeek, si falla usa OpenRouter autom√°ticamente
response = llm_config.call_llm(
    prompt="Your trading prompt...",
    provider="deepseek"  # Fallback a openrouter si falla
)
```

### **Verificar Disponibilidad**
```python
from llm_config import llm_config

available = llm_config.get_available_models()
print(available)
# {
#   "deepseek": {"available": True, "models": [...]},
#   "openrouter": {"available": True, "models": [...]}
# }
```

---

## üîß **Configuraci√≥n Avanzada**

### **Personalizar Temperatura**
```python
# M√°s conservador (trading)
response = get_llm_response(
    prompt="Trading decision...",
    temperature=0.3,  # Menos creativo, m√°s consistente
    provider="deepseek"
)

# M√°s creativo (research)
response = get_llm_response(
    prompt="Find new opportunities...",
    temperature=0.7,  # M√°s exploraci√≥n
    provider="openrouter"
)
```

### **Ajustar Max Tokens**
```python
# Respuestas largas
response = get_llm_response(
    prompt="Deep analysis...",
    max_tokens=3000,  # M√°s detalle
    provider="deepseek"
)

# Respuestas cortas
response = get_llm_response(
    prompt="Quick answer...",
    max_tokens=500,  # Conciso
    provider="openrouter"
)
```

---

## üéØ **Estrategia de Uso Recomendada**

### **DeepSeek Para:**
- ‚úÖ Decisiones de trading cr√≠ticas (deepseek-chat)
- ‚úÖ An√°lisis complejo con razonamiento (deepseek-reasoner)
- ‚úÖ Generaci√≥n de c√≥digo (deepseek-coder)
- ‚úÖ Producci√≥n diaria (r√°pido y econ√≥mico)

### **OpenRouter (Gratis) Para:**
- ‚úÖ **Gemini 2.0 Flash**: Mejor modelo gratis general, multimodal, reasoning
- ‚úÖ **Gemini 2.0 Thinking**: Razonamiento profundo cuando no necesitas tools
- ‚úÖ **Qwen 3 (235B)**: Modelo masivo para an√°lisis complejos
- ‚úÖ **Phi-4**: Modelo peque√±o pero potente, r√°pido
- ‚úÖ **Llama 3.1 (405B)**: Alternativa open-source de alta calidad
- ‚úÖ Testing y desarrollo
- ‚úÖ Backup cuando DeepSeek falla
- ‚úÖ Experimentaci√≥n sin costo

---

## ‚ö†Ô∏è **Troubleshooting**

### **Error: "No API keys configured"**
```bash
# Verificar que .env existe
ls .env

# Verificar contenido
cat .env  # Linux/Mac
type .env  # Windows

# Asegurar que las keys est√°n configuradas
```

### **Error: "DeepSeek API error"**
```bash
# El sistema autom√°ticamente cambia a OpenRouter
# Pero verifica tu key:
python llm_config.py
```

### **Modelos gratuitos lentos**
```bash
# Normal para modelos gratuitos
# Usa DeepSeek para velocidad (muy econ√≥mico)
# OpenRouter es backup/testing
```

---

## üí° **Tips de Optimizaci√≥n**

### **Costos**
```python
# DeepSeek es ~100x m√°s barato que GPT-4
# OpenRouter free models = $0

# Para minimizar costos:
# 1. Usa OpenRouter para desarrollo
# 2. Usa DeepSeek para producci√≥n
# 3. Reserva GPT-4 solo para casos cr√≠ticos
```

### **Performance**
```python
# DeepSeek: ~1-2s respuesta
# OpenRouter Free: ~3-10s respuesta (depende de carga)

# Para tiempo real: DeepSeek
# Para batch processing: OpenRouter OK
```

---

## üîó **Enlaces √ötiles**

- **DeepSeek Docs**: https://platform.deepseek.com/docs
- **OpenRouter Docs**: https://openrouter.ai/docs
- **OpenRouter Free Models**: https://openrouter.ai/models?order=newest&supported_parameters=tools&max_price=0
- **Agno Framework**: https://github.com/agno-agi/agno

---

## üéâ **Ejemplo Completo**

```python
# test_llm_providers.py
from llm_config import llm_config, get_llm_response

# Test 1: DeepSeek (principal)
print("ü§ñ Testing DeepSeek...")
response1 = get_llm_response(
    "Should I buy ABEO stock at $7.50?",
    provider="deepseek",
    model="deepseek-chat"
)
print(f"DeepSeek: {response1}\n")

# Test 2: OpenRouter Free (backup)
print("üåê Testing OpenRouter (free)...")
response2 = get_llm_response(
    "Should I buy ABEO stock at $7.50?",
    provider="openrouter",
    model="llama-3.1-8b"
)
print(f"OpenRouter: {response2}\n")

# Test 3: Fallback autom√°tico
print("üîÑ Testing automatic fallback...")
response3 = llm_config.call_llm(
    "Quick market analysis",
    provider="deepseek"  # Cambia a openrouter si falla
)
print(f"Response: {response3}")
```

---

¬°Ahora tienes **2 proveedores potentes** con fallback autom√°tico y **modelos gratuitos** para desarrollo! üöÄ
